{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ad29422-6c3c-471b-b5ba-620504a7df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7671b20e-4f40-46fd-bc37-4c02c5f8a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  BBC set for major shake-up, claims newspaper L...      2\n",
      "1  Marsh averts cash crunch Embattled insurance b...      2\n",
      "2  Jeter, Yankees Look to Take Control (AP) AP - ...      1\n",
      "3  Flying the Sun to Safety When the Genesis caps...      3\n",
      "4  Stocks Seen Flat as Nortel and Oil Weigh  NEW ...      2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"train.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "print(df.head())\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "571e6285-79f8-476c-bf36-fbfb08923a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization: lowercase + remove stopwords\"\"\"\n",
    "    words = re.findall(r\"\\b[a-z]+\\b\", text.lower())\n",
    "    return [w for w in words if w not in stop_words and len(w) > 2]\n",
    "all_words = []\n",
    "for row in df.itertuples(index=False):\n",
    "    all_words.extend(tokenize(row.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b257b3c-cb43-45ff-9e67-3b485aba4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"min_word_freq\": 2,\n",
    "    \"max_word_freq_ratio\": 0.5,\n",
    "    \"vcand_size\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4a09ec3-805b-4c2f-bcff-5efa66df5f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3089"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter(all_words)\n",
    "total_words = len(all_words)\n",
    "# Filter: not too rare, not too common\n",
    "min_freq = config[\"min_word_freq\"]\n",
    "max_freq = int(total_words * config[\"max_word_freq_ratio\"])\n",
    "\n",
    "filtered_words = [\n",
    "    (word, count) for word, count in word_counts.items()\n",
    "    if min_freq <= count <= max_freq\n",
    "]\n",
    "# Sort by frequency\n",
    "filtered_words.sort(key=lambda x: x[1], reverse=True)\n",
    "top_words_frequency = [word for word, _ in filtered_words]\n",
    "len(top_words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c37eb21-0df2-4e86-b01f-450c670d9530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=config[\"vcand_size\"] * 2,\n",
    "    tokenizer=tokenize,\n",
    "    lowercase=True,\n",
    "    min_df=config[\"min_word_freq\"],\n",
    "    max_df=config[\"max_word_freq_ratio\"]\n",
    ")\n",
    "vectorizer.fit(texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = vectorizer.transform(texts)\n",
    "avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "word_scores = list(zip(feature_names, avg_tfidf))\n",
    "word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "top_words_tfidf = [word for word, _ in word_scores]\n",
    "len(top_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7fa3e5a-c3bf-4244-b178-15319d920c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3089"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_words = {0: [], 1: [], 2: [],3:[]}\n",
    "for text, label in zip(texts, labels):\n",
    "    words = tokenize(text)\n",
    "    class_words[label].extend(words)\n",
    "class_counts = {label: Counter(words) for label, words in class_words.items()}\n",
    "all_words = set()\n",
    "for counts in class_counts.values():\n",
    "    all_words |= set(counts.keys())\n",
    "labels_order = [0,1,2,3]\n",
    "word_scores = []\n",
    "for word in all_words:\n",
    "    freqs = [class_counts[label].get(word, 0) for label in labels_order]\n",
    "    total_freq = sum(freqs)\n",
    "    if total_freq < config[\"min_word_freq\"]:\n",
    "        continue\n",
    "    max_freq = max(freqs)\n",
    "    avg_other = (total_freq - max_freq) / (len(freqs) - 1)\n",
    "    diff_score = (max_freq - avg_other) / total_freq\n",
    "    word_scores.append((word, diff_score, total_freq))\n",
    "word_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "top_words_class_specific = [word for word, _, _ in word_scores]\n",
    "len(top_words_class_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4444840c-e051-443c-8fa1-3d5d45062b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "set_freq = set(top_words_frequency)\n",
    "set_tfidf = set(top_words_tfidf)\n",
    "set_class = set(top_words_class_specific)\n",
    "combined_vocab = list(set_freq & set_tfidf & set_class)\n",
    "print(len(combined_vocab))\n",
    "top_words_combined = random.sample(combined_vocab, config[\"vcand_size\"])\n",
    "print(len(top_words_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a78f774-8a03-41d0-9d38-d21583ec1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vcand_frequency.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in top_words_frequency:\n",
    "        f.write(word + \"\\n\")\n",
    "with open(\"vcand_tfidf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in top_words_tfidf:\n",
    "        f.write(word + \"\\n\")\n",
    "with open(\"vcand_class_specific.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in top_words_class_specific:\n",
    "        f.write(word + \"\\n\")\n",
    "with open(\"vcand.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in top_words_combined:  \n",
    "        f.write(word + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
